# Model basic information
prompt_template: "real_lod/prompts/real_agent.yaml"
template_name: "chatglm"

model_config:
    wrapper_type: "chatglm"
    model_name: "Eureka544/chatglm-6b"
    backend: "huggingface"
    ckpt_path: "back/agent_train/output/agent0_midpromptbalanced_4cls_noconclusion_reformat_newprompt_4gpu/checkpoint-50000/adapter_model.bin"

    peft_config:
      peft_type: "lora"
      task_type: "CAUSAL_LM"
      inference_mode: true
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1

    generate_args:
      max_new_tokens: 1024
      temperature: 0
      do_sample: false
      repetition_penalty: 1.0

    model_args:
      use_safetensors: true
